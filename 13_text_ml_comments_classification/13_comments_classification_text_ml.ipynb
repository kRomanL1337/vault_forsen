{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Содержание<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Подготовка\" data-toc-modified-id=\"Подготовка-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Подготовка</a></span></li><li><span><a href=\"#Обучение\" data-toc-modified-id=\"Обучение-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Обучение</a></span></li><li><span><a href=\"#Выводы\" data-toc-modified-id=\"Выводы-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Выводы</a></span></li><li><span><a href=\"#Чек-лист-проверки\" data-toc-modified-id=\"Чек-лист-проверки-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Чек-лист проверки</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект для «Викишоп»"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интернет-магазин «Викишоп» запускает новый сервис. Теперь пользователи могут редактировать и дополнять описания товаров, как в вики-сообществах. То есть клиенты предлагают свои правки и комментируют изменения других. Магазину нужен инструмент, который будет искать токсичные комментарии и отправлять их на модерацию. \n",
    "\n",
    "Обучим модель классифицировать комментарии на позитивные и негативные. В нашем распоряжении набор данных с разметкой о токсичности правок.\n",
    "\n",
    "Попробуем построить модель со значением метрики качества *F1* не менее 0.75. \n",
    "\n",
    "**План действий по выполнению проекта**\n",
    "\n",
    "1. Загрузим и подготовим данные.\n",
    "2. Обучим разные модели. \n",
    "3. Сделаем выводы.\n",
    "\n",
    "**Описание данных**\n",
    "\n",
    "Данные находятся в файле `toxic_comments.csv`. Столбец *text* в нём содержит текст комментария, а *toxic* — целевой признак."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Подготовка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# помним про PEP-8\n",
    "# импорты из стандартной библиотеки\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:88% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(\"<style>.container { width:88% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#импортируем библиотеки\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import lightgbm as lgb\n",
    "\n",
    "#импортируем встроенный модуль для работы с регулярными выражениями (regular expressions)\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords as nltk_stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm.notebook import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    RandomizedSearchCV,\n",
    "    train_test_split\n",
    ")\n",
    "\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Games\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Games\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Подгрузим необходимые файлы для английской библиотеки лемматизатора:\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#константы\n",
    "\n",
    "RANDOM_STATE = 12345\n",
    "CV = 5 \n",
    "VERBOSE = 1\n",
    "N_JOBS = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    forsen = pd.read_csv('/datasets/toxic_comments.csv')\n",
    "except:\n",
    "    forsen = pd.read_csv('C:/Users/Games/Downloads/toxic_comments.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим общую информацию по датасету."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 159292 entries, 0 to 159291\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count   Dtype \n",
      "---  ------      --------------   ----- \n",
      " 0   Unnamed: 0  159292 non-null  int64 \n",
      " 1   text        159292 non-null  object\n",
      " 2   toxic       159292 non-null  int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 3.6+ MB\n"
     ]
    }
   ],
   "source": [
    "forsen.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text  toxic\n",
       "0           0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1           1  D'aww! He matches this background colour I'm s...      0\n",
       "2           2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3           3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4           4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу бросается в глаза лишний столбец 'Unnamed: 0', который, по всей видимости, дублирует индексы. Избавимся от него т.к. он не несёт никакой смысловой нагрузки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "forsen = forsen.drop('Unnamed: 0', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  toxic\n",
       "0  Explanation\\nWhy the edits made under my usern...      0\n",
       "1  D'aww! He matches this background colour I'm s...      0\n",
       "2  Hey man, I'm really not trying to edit war. It...      0\n",
       "3  \"\\nMore\\nI can't make any real suggestions on ...      0\n",
       "4  You, sir, are my hero. Any chance you remember...      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ни пропусков, ни дубликатов в датасете не имеется."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для работы с регулярными выражениями в Python есть встроенный модуль re (сокр. от regular expressions). Создадим функцию, которая очистит текст для будущей лемматизации.  \n",
    "А также создадим функцию для токенизации и лемматизации текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_text(text):\n",
    "    \n",
    "    new_text = re.sub(r'[^a-zA-Z ]', ' ', text.lower())\n",
    "    new_text = new_text.split()\n",
    "    new_text = \" \".join(new_text)\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-12 16:49:12.151049: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-08-12 16:49:12.151218: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-08-12 16:49:16.829273: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudart64_110.dll'; dlerror: cudart64_110.dll not found\n",
      "2023-08-12 16:49:16.830353: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublas64_11.dll'; dlerror: cublas64_11.dll not found\n",
      "2023-08-12 16:49:16.831266: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cublasLt64_11.dll'; dlerror: cublasLt64_11.dll not found\n",
      "2023-08-12 16:49:16.832446: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cufft64_10.dll'; dlerror: cufft64_10.dll not found\n",
      "2023-08-12 16:49:16.833471: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'curand64_10.dll'; dlerror: curand64_10.dll not found\n",
      "2023-08-12 16:49:16.834299: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusolver64_11.dll'; dlerror: cusolver64_11.dll not found\n",
      "2023-08-12 16:49:16.835080: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cusparse64_11.dll'; dlerror: cusparse64_11.dll not found\n",
      "2023-08-12 16:49:16.835914: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'cudnn64_8.dll'; dlerror: cudnn64_8.dll not found\n",
      "2023-08-12 16:49:16.835935: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1850] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.2.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.6.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.6.0/en_core_web_sm-3.6.0-py3-none-any.whl (12.8 MB)\n",
      "                                              0.0/12.8 MB ? eta -:--:--\n",
      "                                              0.0/12.8 MB ? eta -:--:--\n",
      "                                              0.1/12.8 MB 1.3 MB/s eta 0:00:10\n",
      "     -                                        0.4/12.8 MB 3.3 MB/s eta 0:00:04\n",
      "     --                                       0.7/12.8 MB 4.4 MB/s eta 0:00:03\n",
      "     ----                                     1.4/12.8 MB 6.6 MB/s eta 0:00:02\n",
      "     ------                                   2.0/12.8 MB 7.6 MB/s eta 0:00:02\n",
      "     -------                                  2.6/12.8 MB 8.2 MB/s eta 0:00:02\n",
      "     ---------                                3.1/12.8 MB 8.7 MB/s eta 0:00:02\n",
      "     -----------                              3.7/12.8 MB 9.0 MB/s eta 0:00:02\n",
      "     -------------                            4.2/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     --------------                           4.8/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     ----------------                         5.3/12.8 MB 9.8 MB/s eta 0:00:01\n",
      "     ------------------                       5.9/12.8 MB 9.9 MB/s eta 0:00:01\n",
      "     -------------------                      6.4/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     ---------------------                    7.0/12.8 MB 10.2 MB/s eta 0:00:01\n",
      "     -----------------------                  7.5/12.8 MB 10.3 MB/s eta 0:00:01\n",
      "     -------------------------                8.0/12.8 MB 10.5 MB/s eta 0:00:01\n",
      "     --------------------------               8.6/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     --------------------------               8.6/12.8 MB 10.4 MB/s eta 0:00:01\n",
      "     ----------------------------             9.0/12.8 MB 10.0 MB/s eta 0:00:01\n",
      "     ----------------------------             9.1/12.8 MB 9.5 MB/s eta 0:00:01\n",
      "     -----------------------------            9.3/12.8 MB 9.3 MB/s eta 0:00:01\n",
      "     -----------------------------            9.5/12.8 MB 9.0 MB/s eta 0:00:01\n",
      "     ------------------------------           9.7/12.8 MB 8.9 MB/s eta 0:00:01\n",
      "     ------------------------------           9.9/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     -------------------------------          10.1/12.8 MB 8.5 MB/s eta 0:00:01\n",
      "     --------------------------------         10.3/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     --------------------------------         10.5/12.8 MB 8.7 MB/s eta 0:00:01\n",
      "     ---------------------------------        10.7/12.8 MB 8.6 MB/s eta 0:00:01\n",
      "     ----------------------------------       10.9/12.8 MB 8.4 MB/s eta 0:00:01\n",
      "     ----------------------------------       11.1/12.8 MB 8.3 MB/s eta 0:00:01\n",
      "     -----------------------------------      11.4/12.8 MB 8.1 MB/s eta 0:00:01\n",
      "     ------------------------------------     11.6/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     ------------------------------------     11.8/12.8 MB 7.8 MB/s eta 0:00:01\n",
      "     -------------------------------------    12.0/12.8 MB 7.6 MB/s eta 0:00:01\n",
      "     --------------------------------------   12.2/12.8 MB 7.4 MB/s eta 0:00:01\n",
      "     --------------------------------------   12.5/12.8 MB 7.2 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.7/12.8 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------  12.8/12.8 MB 7.0 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 12.8/12.8 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: spacy<3.7.0,>=3.6.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from en-core-web-sm==3.6.0) (3.6.1)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.4)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.0.9)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.7)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.0.8)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.12)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.4.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.0.9)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.9.0)\n",
      "Requirement already satisfied: pathy>=0.10.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.10.2)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (6.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.65.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.20.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.28.2)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.10.9)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (67.6.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (23.0)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.3.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (4.5.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2022.12.7)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.7.10)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from thinc<8.2.0,>=8.1.8->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.1.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (0.4.6)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (8.1.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\games\\anaconda3\\envs\\ds_practicum_env\\lib\\site-packages (from jinja2->spacy<3.7.0,>=3.6.0->en-core-web-sm==3.6.0) (2.1.2)\n",
      "\u001b[38;5;2m[+] Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize(text):\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    return \" \".join([token.lemma_ for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lemmatizer = WordNetLemmatizer()\n",
    "#\n",
    "#def lemmatize(text):\n",
    "#    \n",
    "#    word_list = nltk.word_tokenize(text)\n",
    "#    return ' '.join([lemmatizer.lemmatize(w) for w in word_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сразу создадим доп. столбец в датасете с обработанными комментариями (правками)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe98677bda7f4843a0976b7cf4574114",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forsen['lemm_text'] = forsen['text'].progress_apply(clear_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7e3324348c4faaa87c8958d9ccd877",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/159292 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "forsen['lemm_text'] = forsen['lemm_text'].progress_apply(lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         explanation why the edit make under my usernam...\n",
       "1         d aww he match this background colour I m seem...\n",
       "2         hey man I m really not try to edit war it s ju...\n",
       "3         more I can t make any real suggestion on impro...\n",
       "4         you sir be my hero any chance you remember wha...\n",
       "                                ...                        \n",
       "159287    and for the second time of ask when your view ...\n",
       "159288    you should be ashamed of yourself that be a ho...\n",
       "159289    spitzer umm there s no actual article for pros...\n",
       "159290    and it look like it be actually you who put on...\n",
       "159291    and I really don t think you understand I come...\n",
       "Name: lemm_text, Length: 159292, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen['lemm_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на соотношение объёма положительного класса к отрицательному."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    143106\n",
       "1     16186\n",
       "Name: toxic, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forsen['toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Процент объектов класса 1 к общему объёму датасета: 10.16%\n"
     ]
    }
   ],
   "source": [
    "print(f\"Процент объектов класса 1 к общему объёму датасета: {(sum(forsen['toxic']) / len(forsen) * 100):.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы алгоритмы умели определять тематику и тональность текста, их нужно обучить на корпусе. Это набор текстов, в котором эмоции и ключевые слова уже размечены."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделим датасет на тестовую и тренировочную выборку, размер тестовой выборки - 10% от общих данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92651     a thing at time the moans of birkin bardot be ...\n",
       "57776     re suicide I m go to take my advice and shrug ...\n",
       "155506                                       archive to dec\n",
       "14143     hello the tag be add because the article start...\n",
       "128218    good morning look where the satanic comment ip...\n",
       "                                ...                        \n",
       "9205      yet again I have repetedlty ask you not to add...\n",
       "19763     my edit be an addition why would I modify the ...\n",
       "136558    a reference that may interest you hello zleitz...\n",
       "75564     I m not try to make a point or anything except...\n",
       "55751     talk grasshopper scout I move your comment fro...\n",
       "Name: lemm_text, Length: 143362, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features, test_features, train_target, test_target = train_test_split(\n",
    "    forsen.drop('toxic', axis=1),\n",
    "    forsen['toxic'],\n",
    "    test_size=0.1,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=forsen['toxic'] # стратифицируем текст, чтобы выборки были более сбалансированы\n",
    ")\n",
    "\n",
    "# вытаскиваем корпусы\n",
    "corpus_train = train_features['lemm_text']\n",
    "corpus_test = test_features['lemm_text']\n",
    "corpus_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((143362,), (15930,))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_train.shape, corpus_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мешок слов учитывает частоту употребления слов. Оценка важности слова определяется величиной TF-IDF.  \n",
    "То есть TF отвечает за количество упоминаний слова в отдельном тексте, а IDF отражает частоту его употребления во всём корпусе.  \n",
    "Воспользуемся TfidfVectorizer и, чтобы почистить мешок слов, добавим в него стоп-слова:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(nltk_stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (143362, 142726)\n",
      "Размер матрицы: (15930, 142726)\n"
     ]
    }
   ],
   "source": [
    "count_tf_idf = TfidfVectorizer(stop_words=stopwords) # подгружаем счетчик и задаём стоп-слова\n",
    "tf_idf_train = count_tf_idf.fit_transform(corpus_train) # обучаем и трансформируем\n",
    "tf_idf_test = count_tf_idf.transform(corpus_test) # трансформируем тестовую без обучения\n",
    "\n",
    "print(\"Размер матрицы:\", tf_idf_train.shape)\n",
    "print(\"Размер матрицы:\", tf_idf_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Напишем функцию для кросс-валидации наших моделей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, parameters):\n",
    "    pipeline = Pipeline([\n",
    "        ('tfidf', TfidfVectorizer()),\n",
    "        ('model', model)])\n",
    "    model_random = RandomizedSearchCV(\n",
    "        pipeline,\n",
    "        cv=CV,\n",
    "        n_jobs=N_JOBS,\n",
    "        param_distributions=parameters,\n",
    "        scoring='f1',\n",
    "        verbose=VERBOSE\n",
    "    )\n",
    "    \n",
    "    start = time()\n",
    "    model_random.fit(train_features['lemm_text'], train_target)\n",
    "    print('RandomizedSearchCV подбирал параметры %.2f секунд' %(time() - start))\n",
    "    \n",
    "    # высчитаем метрику\n",
    "    f1 = model_random.best_score_\n",
    "    \n",
    "    print('Лучшие параметры:', model_random.best_params_)\n",
    "    print('F1 обученной модели:', f1)\n",
    "\n",
    "    return model_random, f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model__C\": [0.01, 0.1, 1, 10, 100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n",
      "RandomizedSearchCV подбирал параметры 127.93 секунд\n",
      "Лучшие параметры: {'model__C': 10}\n",
      "F1 обученной модели: 0.7882215790349266\n"
     ]
    }
   ],
   "source": [
    "lr_random, lr_f1 = train_model(lr, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Random Forest Classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    \"model__max_depth\": [300, 310],\n",
    "    \"model__n_estimators\": [12, 14],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "RandomizedSearchCV подбирал параметры 1028.25 секунд\n",
      "Лучшие параметры: {'model__n_estimators': 14, 'model__max_depth': 300}\n",
      "F1 обученной модели: 0.5620716208369567\n"
     ]
    }
   ],
   "source": [
    "rfc_random, rfc_f1 = train_model(rfc, param_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 14567, number of negative: 128795\n",
      "[LightGBM] [Warning] Auto-choosing col-wise multi-threading, the overhead of testing was 2.082222 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 615353\n",
      "[LightGBM] [Info] Number of data points in the train set: 143362, number of used features: 10710\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.101610 -> initscore=-2.179463\n",
      "[LightGBM] [Info] Start training from score -2.179463\n",
      "RandomizedSearchCV подбирал параметры 604.07 секунд\n",
      "Лучшие параметры: {'model__max_depth': 25, 'model__learning_rate': 0.3}\n",
      "F1 обученной модели: 0.7773540654242062\n",
      "CPU times: total: 2min 21s\n",
      "Wall time: 10min 4s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rand_lgbm_param = {\n",
    "    \"model__max_depth\": [15, 25],\n",
    "    \"model__learning_rate\": [0.1, 0.3]\n",
    "}\n",
    "\n",
    "gbm = lgb.LGBMClassifier()\n",
    "\n",
    "gbm_random, gbm_f1 = train_model(gbm, rand_lgbm_param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрели три модели, ниже напишем выводы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выводы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LogisticRegression</th>\n",
       "      <td>0.788222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RandomForestClassifier</th>\n",
       "      <td>0.562072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LGBM</th>\n",
       "      <td>0.777354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              F1\n",
       "LogisticRegression      0.788222\n",
       "RandomForestClassifier  0.562072\n",
       "LGBM                    0.777354"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(\n",
    "    [lr_f1, rfc_f1, gbm_f1], \n",
    "    index=['LogisticRegression', 'RandomForestClassifier', 'LGBM'], \n",
    "    columns=['F1']\n",
    ")\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из трёх моделей лучшую метрику показала логистическая регрессия. Далее протестируем её с уже известными нам наилучшими гиперпараметрами на тестовой выборке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 лучшей модели на тестовой выборке: 0.7983651226158036\n",
      "CPU times: total: 844 ms\n",
      "Wall time: 884 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "lr_prediction = lr_random.predict(test_features['lemm_text'])\n",
    "metric_test = f1_score(test_target, lr_prediction)\n",
    "print('F1 лучшей модели на тестовой выборке:', metric_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Значение метрики на тестовой выборке (0,80) получилось выше требуемого порога в 0,75. Задание выполнено."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузив датасет, мы почистили тексты правок и лемматизировали их. Обработанные данные поместили в новый столбец датасета.  \n",
    "Чтобы алгоритмы умели определять тематику и тональность текста, мы создали признаки и обучили их на корпусе. Это набор текстов, в котором эмоции и ключевые слова уже размечены. Перед этим мы разделили датасет на тренировочную и тестовую выборки.\n",
    "\n",
    "Затем мы написали функцию для кросс-валидации наших моделей. Выбор пал на три модели:\n",
    "* Логистическая регрессия\n",
    "* Случайный лес\n",
    "* LightGBM (градиентный бустинг)\n",
    "\n",
    "Наилучшую метрику показала первая, 0,79.\n",
    "\n",
    "Далее мы перешли непосредственно к тестированию лучшей модели на тестовой выборке, в результате чего **получили метрику F1 выше, чем того от нас требовалось, а именно 0,80 при необходимом минимальном пороге в 0,75.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Содержание",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "302.391px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
